---
title: "Problem Set 4 Empirical Methods"
author: "Patrick Glettig (13-252-143), Carmine Ragone (14-995-021)"
date: "16.12.2018"
header-includes:
  - \usepackage{xcolor}
output:
  html_document:
    df_print: paged
  pdf_document:
    extra_dependencies: dcolumn
---

```{r global options, include=FALSE}
library(knitr)
library(Cairo)
library(extrafont)
library(latex2exp)
library(readstata13)
library(Hmisc)
library(lmtest)
library(stargazer)
library(kableExtra)
library(ggplot2)
library(data.table)
library(tidyr)
library(formatR)
library(dplyr)
library(car)
library(sjmisc)
library(plm)
library(AER)
library(ivpack)


#install.packages("pacman")
#pacman::p_load(knitr, Cairo, extrafont,latex2exp,readstata13,Hmisc,lmtest,stargazer,kableExtra,ggplot2,data.table,tidyr,formatR,dplyr,car,sjmisc,plm,AER,ivpack) 


knitr::opts_chunk$set(echo = TRUE, warning = FALSE,fig.show = "hold", collapse = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)

sysinf <- Sys.info()
if(sysinf['sysname'] == "Windows"){
  extrafont::loadfonts(device="win")
  #opts_chunk$set(dev = "CairoPDF")
}
```


# Paper and Pencil Questions

## Motivating Linear Panel Data

### (a)

First follow the hint and write down the function in level terms:
\begin{align*}
y_{it} &= x_{it}\beta+\alpha_i+\epsilon_{it} \\
\log{y_{it}}&=\log{x_{it}}\beta+\log{\alpha_i}+\epsilon_{it} \\
e^{\log{y_{it}}}&=e^{\log{x_{it}}\beta+\log{\alpha_i}+\epsilon_{it}} \\
y_{it} &= x_{it}^{\beta} \alpha_{i} e^{\epsilon_{it}}
\end{align*}

The farmer will maximize his profit by changing the input factor $x_{it}$, therefore he maximizes his expected profit function with respect to $x_{it}$. We first find the profit function

\begin{align*}
\pi(x_{it},\alpha_{i})&=P_t(y_{it})-W_t(x_{it}) \\
&=P_t (x_{it}^{\beta} \alpha_{i} e^{\epsilon_{it}}) - W_t x_{it}
\end{align*}

To find the expected profit function, we use the hint $\lambda=Ee^{\epsilon_{it}}$:

\begin{align*}
E(y_{it}|x_{it}) &= E(x_{it}^{\beta} \alpha_i e^{\epsilon_{it}}) \\
&= x_{it}^{\beta} \alpha_i E(e^{\epsilon_{it}}) \\
&= x_{it}^{\beta} \alpha_i \lambda
\end{align*}

Now maximize the profit function using the hint above:

\begin{align*}
\max{E(\pi_{it}|x_{it})} &= E(P_t y_{it} - W_t x_{it} | x_{it}) \\
&= P_t (x_{it}^{\beta} \alpha_{i} \lambda) - W_t x_{it}
\end{align*}

\begin{align*}
\frac{\partial \pi_{it}}{\partial x_{it}}= \beta P_t x_{it}^{\beta-1} \alpha_i \lambda - W_t &{\overset{!}{=}} 0 \tag{F.O.C.} \\
(x_{it})^{\beta-1} &= \frac{W_t}{\beta P_t \alpha_i \lambda} \\
x_{it} &= \left(\frac{\beta P_t \alpha_i \lambda}{W_t}\right)^{\beta-1}
\end{align*}

The labor demand $x_{it}$ depends on wages $W_t$, the labor demand coefficient $\beta$, prices $P_t$, soil quality $\alpha_i$ and rainfall $\epsilon_{it}$.

**Economic intuition:**
If prices are rising, farmers should produce more (this also aligns with any basic supply function). When wages rise, the optimal output will be less. The better the soil quality and rainfall, the more working hours are required (note that here the interpretation depends on how these variables are measured). It is worth to mention that, if $\epsilon_{it} \sim N(0,1)$ then $E(\epsilon_{it})=0$ and $\lambda=1$, so rainfall in expectation does not affect labor demand.  Nevertheless we assume that $\beta$ is some kind of input-output elasticity, a factor by how much $labor$ is turned into $output$. If this factor increases, it pays for the farmer to demand more working hours.

$\beta$ is an input-output elasticity, a factor by how much $labor$ is turned into $output$. Generally in labor economics, $\beta \in (0,1)$. If we assume this is the case, the lower $beta$, the more $labor$ is demanded. This makes sense as with a low elasticity, more workers are needed to achieve the same output. If $\beta=1$, $x_{it}=1$ and if $\beta>1$, $W_t$ becomes the numerator divided by $\beta P_t \alpha_i \lambda$ while the whole fraction is taken to the power of some positive number. Which means that the whole interpretation described in the paragraph above turns around (e.g. increase becomes decrease). Again, this makes sense in an economic way as in such a case with one unit of $labor$, more than one unit of $output$ is produced.

### (b)

Any OLS-equation is required to be linear to recover a consistent estimate. However, production functions can hardly assumed to be linear. In the given example, the farmer chooses $x_{it}$ by a non-linear expression of $\beta,P_t, \alpha_i$ and $\lambda$. Therefore, it is likely that the assumption of linearity is violated in the case at hand.

### (b) - Carmine
It is possible to recover a consistent estimate for $\beta$ running a pooled OLS only under the assumption of *uncorrelated effects* ($Cov(x_{it}, \alpha_i)=0$) for each $t=1,..,T$. Formally, the Contemporaneous Exogeneity should be fulfilled as well, but we will assume it is the case given that "$\epsilon_{it}$ is iid and independent of
everything in the model." As seen and demonstrated in 1.a the farmer will maximize its profit by choosing the number of workers also given the quality of the soil (\alpha_i) - e.g. less people if the soil has an high quality. Therefore, it is likely that the assumption of linearity is violated in the case at hand.

### (c)
### (d)
i) Random Effects:
- *Strict Exogeneity* -> $Cov(x_{it}, \epsilon_{it})=0$ for each $s,t=1,..,T$
- *uncorrelated effects* -> $Cov(x_{it}, \alpha_i)=0$) for each $t=1,..,T$.
ii) Fixed Effects:
- *Strict Exogeneity* -> $Cov(x_{it}, \epsilon_{it})=0$ for each $s,t=1,..,T$
- *Arbitrary Effects* -> relationship between $\alpha_i$ and $x_{it}$ is completely unrestricted
iii) First Differences:
- $Cov(\Delta x_{it})$

# Empirical Applications

## IV Regression

```{r results='hide'}
BCHHS <- fread('BCHHS.csv')
head(BCHHS) # Check the format
BCHHS$lnearn <- log(BCHHS$earning) #create log earnings
BCHHS$agesq <- (BCHHS$age)^2 #create age squared
dim(BCHHS) #Check length
summary(BCHHS) #Explore the data
```

### a)

```{r, message=FALSE, warning=FALSE, results='asis'}
Model1 = lnearn ~ highqua + age + agesq

lm1 <- lm(Model1, data = BCHHS, x=TRUE)
iv1 <- ivreg(lnearn ~ highqua + age + agesq | twihigh + age + agesq, data = BCHHS, x=TRUE)
```
```{r tab1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer(lm1, iv1, title="Regression Results Earnings with and without IV", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Log Wage"),
 covariate.labels=c("Education", "Age", "Age squared", "Constant"),
 model.names = FALSE,
 column.labels = c("OLS", "IV"),
 no.space=TRUE)
```

i)
The estimates are equivalent to the most part (see Table 1). Any discrepancy however is not serious because the estimates confirm the paper's results; an increase in education (also in the education instrument) increases earnings, an increase in age goes along with an increase in earnings and an increase in age squared causes a decrease in earnings. All estimates are still significant, pointing to a causal effect. The results also match the pattern of other research mentioned in the paper. Therefore, this discrepancy is not serious.

ii)
The constant could be interpreted as average log earnings when everything else is zero, in other words the average log earnings of newborns with zero education. Since this does not really make sense, there is no important information contained in that coefficient.

iii)

*First Model:* A one year increase in education increases earnings by approximately 7.7% (or technically, by $100(e^{\beta_2}-1)\%$, `r 100*(exp(lm1$coefficients[2])-1)`%. A one year increase in age increases earnings by approximately 7.8%. However, age has a decreasing marginal effect showed in age squared: a one-unit increase in age squared decreases earnings by approximately 0.1%. The turning point of age can be found by $-\frac{\textrm{Age}}{2\textrm{Age squared}}=39$, after this age the effect of age on earnings becomes negative.

*Second Model:* The interpretation is the same as for the first model (with slightly different numbers), only that instead of actual education, an instrumental variable is used: a one year increase in reported education increases earnings by approximately 8.7%. The turning point is one year earlier, at 38 years.

### b)

i)
- Measurement Error: As the authors of the study mention on page 3, 10% of variation in schooling is due to measurement   error. This especially becomes important with the fact that the measure is self-reported.
- Omitted Variable Bias: the model wants to estimate the effect of schooling on income. However, without controls years of education could simply be a proxy for ability, as more intelligent individuals attend university and therefore have more years of education.

ii)
- Measurement Error: assuming that we have a classical measurement error, this would result in an attenuation bias of the independent variable, i.e. a negative bias (the coefficient is smaller than it actually is)

- OVB: Ability is likely to be positively correlated with earnings since higher skilled individuals can take more demanding jobs (hence earn more). Education will also be positively correlated with ability (see reasoning above). Therefore, the sign of the bias is positive.

iii)
Instrument relevance is given by:

$$\frac{1}{N}Z'X=\frac{1}{N}\sum_{i=1}^{N}z_ix_i' \space \xrightarrow{p} \space E(z_ix_i') \equiv\Sigma zx \neq0$$

And instrument exogeneity by:

$$\frac{1}{N}Z'\epsilon=\frac{1}{N}\sum_{i=1}^{N}z_i \epsilon_i'\space \xrightarrow{p} \space E(z_i\epsilon)=0$$

For relevance, both reasons can be evaluated at once: it is save to assume that $twihigh$ is correlated with $highqua$, so the instrument $z$ moves the endogenous $x_i$, it therefore is relevant. Regarding exogeneity, the answers differ on the reason:
- Measurement Error: it is hard imagine that the error $\epsilon_i$ is correlated with the twin's estimates $highqua$. This makes $highqua$ a relevant and exogenous instrument regarding measurement error and should remove the negative bias from measurement error.
- OVB: the twin's estimate $highqua$ still contain the omitted variable $ability$, therefore $Cov(z_i,\epsilon_i)\neq0$ which makes the instrument endogenous. In other words, using $highqua$ as an instrument will remove the positive bias from measurement error but not the negative bias from the omitted variable $ability$.

iv.)
The coefficient of education with the endogenous measure $highqua$ is 0.077 wheras with the IV regression it increases to 0.087. With the reasoning above, the coefficient on education of the IV is expected to increase as it removes (at least partly) negative the measurement error but not the positive error due to the omitted $ability$. If we compare the same coefficients in the author's table, we see that the increase in $education$ is confirmed by their results. The coefficient however should still be positively biased by the omitted variable $ability$.

v.)

```{r, message=FALSE, warning=FALSE, results='asis'}
FSR <- highqua ~ twihigh

lmFSR <- lm(FSR, data = BCHHS, x=TRUE)
```
```{r tab2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer(lmFSR, title="First Stage regression IV", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Education"),
 covariate.labels=c("Twin estimate of education"),
 omit.stat=c("all"),
 report = ("vc*t"),
 no.space=TRUE)
```

The working definition of a "weak" estimator is an estimator with an F-statistic below 10, or a t-statistic below 3.16. With an t-statistic of $331.9>18.219$, the instrument is not considered as "weak" by the working definition.

vi.)
The problem with the instrument is that it sufficiently shifts $x_i$, but it does not remove the problem of the omitted variable $ability$. It is quite possible that $ability$ is not the only omitted variable, maybe the wage is also influenced by regional factors, state of the economy and so on. In an ideal world, the instrument would shift only education while all other variables remain constant (i.e. ability, regional factors etc.). Therefore, we believe that the education coefficient of the IV regression is at best an "upper bound" of the effect of education on wages: it is likely that still includes positive bias but it improves the credibility by removing the measurement error.

### c)

Reshape the data:

```{r}
BCHHS_wide <- reshape(BCHHS, timevar = "twinno", idvar = "family", direction="wide")
BCHHS_wide$dlnearn <- BCHHS_wide$lnearn.1-BCHHS_wide$lnearn.2
BCHHS_wide$dhigh <- BCHHS_wide$highqua.1-BCHHS_wide$highqua.2
BCHHS_wide$dtwihigh <- BCHHS_wide$twihigh.1-BCHHS_wide$twihigh.2
```

Generate the regressions:

```{r, message=FALSE, warning=FALSE, results='asis'}
Model2 = dlnearn ~ dhigh + 0
Model3 = dlnearn ~ dhigh

lm2 <- lm(Model2, data = BCHHS_wide, x=TRUE)
iv2 <- ivreg(dlnearn ~ dhigh + 0 | dtwihigh + 0, data = BCHHS_wide, x=TRUE)
lm3 <- lm(Model3, data = BCHHS_wide, x=TRUE)
iv3 <- ivreg(dlnearn ~ dhigh | dtwihigh, data = BCHHS_wide, x=TRUE)
```
```{r tab3, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer(lm2, iv2, lm3, iv3, title="Regression Results Earnings within twins with and without IV", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Log Wage Difference"),
 model.names = FALSE,
 column.labels = c("OLS", "IV", "OLS", "IV"),
 covariate.labels=c("Education"),
 omit.stat=c("all"),
 no.space=TRUE)
```

i.)
Because the difference in age measured in years between two twins is always zero (the current record age difference between twins is 85 days, see [\textcolor{blue}{this reference}](https://www.nursingschools.net/blog/2010/10/20-amazing-facts-about-twins/)). Therefore, all values for $age$ and $agesq$ would be zero and we would include a variable without any variability.

ii.)
The constant is the average log wage difference if all $\beta$ are set to 0. The constant becomes redudant if we assume that any difference in the dependent variable can only be caused by one of the independent variables. By taking the differences between twins, we control for any OVB that is caused by $ability$, as the background and genetic setting from twins are identical. Therefore, we would expect that any difference in wages can only be caused by a difference in education so we construct the regression in a way the difference in wage is zero if education is also zero.

The estimated coefficient might have an insignificant effect as almost all of the factors affecting wage should be included by the independent variable education and the fact that differences between twins are used. By the reasoning above the constant should also be close to zero, as most differences in wages can only be explained due to different levels of $education$.

As expected, the constant coefficients in Table 3 become insignificant when using differences between twins as compared to the results in the pooled regression (see Table 1). The values of the constant are also close to zero which supports the reasoning to use a model without a constant.

iii.)
By taking the difference in education between twins, differences in education can no longer be explained through $ability$ as both twins should have the same genetic ability and socio-economic background. Education is now simply the difference in school years and no longer the difference in school years and $ability$ (as it was in the pooled OLS).

This directly adresses the endogeneity concern of the OVB and as expected the coefficient on education decreases from 0.077 to 0.039 (in IV from 0.087 to 0.078) which confirms the expectation of a positive bias due to OVB.

We also note that using IV still removes the negative bias of measurement error since the coefficient of education increases in the regression with the instrumental variable.

iv.)
Combining the within-twins and IV approach, both sources of bias (measurement error and OVB) can be removed. By using such an approach, one should receive the causal relationship between education and $dlnearn$ and the estimate can be interpreted as the return of education to earnings. Another advantage comes by comparing the OLS and IV regression; the difference between the education coefficients hints at the size of the measurement error bias.

v.)
Taking the differences between twins should remove any ability bias as the twins have the same genetic and socio-economic background. Nevertheless, measurement error might still be a problem as there is no reason to assume that the twin's estimate of education should suffer from less measurement error than the education reported from the individual itself. The estimates are certainly more credible than the ones from a pooled OLS, however to obtain the true causal effect of education it is necessary to use a variable that does not suffer from measurement error.

### d)

```{r}
BCHHS_wide$absearn <- abs(BCHHS_wide$earning.1-BCHHS_wide$earning.2)
BCHHS_wide_no_outliers <- dplyr::filter(BCHHS_wide, absearn <= 60)

#if 4 families were removed, this should equal 4:
nrow(BCHHS_wide)-nrow(BCHHS_wide_no_outliers)
```

Run the regressions:

```{r, message=FALSE, warning=FALSE, results='asis'}
Model4 = dlnearn ~ dhigh + 0
Model8 = dlnearn ~ dtwihigh + 0

lm4 <- lm(Model4, data = BCHHS_wide_no_outliers, x=TRUE)
iv4 <- ivreg(dlnearn ~ dhigh + 0 | dtwihigh + 0, data = BCHHS_wide_no_outliers, x=TRUE)
```
```{r tab4, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer(lm4, iv4, title="Regression Results Earnings without outliers", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Log Wage Difference"),
 model.names = FALSE,
 column.labels = c("OLS", "IV"),
 covariate.labels=c("Education", "Education Instrument"),
 omit.stat=c("all"),
 no.space=TRUE)
```

i.)
The coefficients decrease from 0.039 to 0.028 (OLS) and from 0.077 to 0.036, hence the magnitude decreases. Also, while the estimates were significant at the 10 (OLS) and 5 (IV) percent level, both OLS and IV are now not significant at the 10 percent level.

Therefore, our results suggest that removing the outliers decreases the magnitude of education return on wages and its significance. This is in line with the paper "Returns to education: Evidence from UK twins: A comment."

ii.)
The interpretation of the coefficients does not change except for the numbers, a 1-year increase in education difference increases wages by approximately 2.8% (OLS). However, the coefficient has become insignificant and can no longer be interpreted as a casual relationship between earnings and education. It seems that most of the variance in education is driven by outliers with a large wage difference, after removing them the reduced variance causes the standard error to increase (relative to the coefficient), resulting in an insignificant coefficient.

iii.)
Dropping the outliers decreases the variance in $dlnearn$. Many twins might already be earning a similar wage. Therefore, outliers are valuable observations which add to the variance in earnings differences between twins. If such differences are driven by a difference in education, they confirm the causal effect which the authors want to prove.

We can check whether this is the case by looking at the outliers:
```{r}
dplyr::filter(BCHHS_wide, absearn >= 60)%>%select(c(dlnearn,absearn,dhigh,dtwihigh))
```
As confirmed by the data above, a difference in earnings is not always driven by a difference in education. It could be that this difference roots in different fields of occupation. Under these circumstances, it might be more sensible to remove the outliers from the data as they distort the point estimates.

## Basic Panel Data Methods

```{r results='hide'}
Gasoline <- fread('Gasoline.csv')
head(Gasoline) # Check the format
dim(Gasoline) #Check length
summary(Gasoline) #Explore the data
```

### (a)
$ln\left(\dfrac{Gas}{Car}\right)_{it}$ is the gasoline consume per car in the 18 OECD countries over the 19 years. The variables $ln\left(\dfrac{Y}{N}\right)_{it}$  (per capita income) and $ln\left(\dfrac{P_{MG}}{P_{GDP}}\right)_{it}$ (gasoline price) are relevant because they reflect the utilization of autos. In specific, countries with an higher per capita incame use the car more because they can afford to own a car and to use it often. On the other hand, in countries with lower income per capita people will prefer to use the cheaper transportation means when possible. Thus, a positive sign is expected for this variable.[^1] The last variable $ln\left(\dfrac{Car}{N}\right)_{it}$ captures the stock of cars per capita. The rising stock of cars per capita is likely to lead to reduced auto utilization. This is pretta intuitive: (using the same example of the autors of the paper) the two-car family does not drive twice the miles of a one-car family.

[^1]:However, it might be possible to have a negative sign. Assuming that countries with an higher income pro capita have better public transport and more urbanized city centers -e.g Switzerland- people will be more willing to use more public transportations or bike to avoid traffic jam. Thus, the gasoline consume decreases with the increase with the increase of income per capita. 

### (b)
```{r results='hide'}
Gasoline <- fread('Gasoline.csv')
#Y <- cbind(c,data=Gasoline)
#X <- cbind(y,p,car,data=Gasoline)

# Set data as panel data
pdata <- pdata.frame(Gasoline, index=c("co","year"))

# Pooled OLS estimator
pooling <- plm(c ~ y+p+car, data=pdata, model= "pooling")

```
```{r tab5, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer(pooling, type="latex", title="Pooled OLS Results",
          dep.var.labels=c("Gas per car consumption"),
          covariate.labels=c("Per capita income", "Gasoline price","Cars per capita"), align=TRUE)
```
### (c)
The estimating equation for a general Pooled OLS is $y_{it} = x'_{it} \beta + \alpha_{i} + \upsilon_{it}$ where the dummy variables $d_{t}$ are included into $x_{it}$ and the error term is $\upsilon_{it} =\alpha_{i} + \epsilon_{it}$ . This means that we are ignoring the unobserved heterogeneity $\alpha_{i}$ - which representes some individual caracteristic costant across time (this is why there is only "i" and not "t" associated to this term). Therefore, the assumptions behind the Pooled OLS to have a consistent estimator are:

- Contemporaneous Exogeneity 
\begin{align*}
Cov(x_{it},\epsilon_{it})= 0 \quad \text{with} \quad t=1,...,T
\end{align*}

- and Uncorrelated Effects
\begin{align*}
Cov(x_{it},\alpha_{i})= 0 \quad \text{with} \quad t=1,...,T
\end{align*}

In this context the uncorrelated effects assumption is not likely to be satisfided. For instance, factors such as the average distance from working place can be correlated with the number of cars a family ownes. Given that in an average family with 3 membes, 2 of them are working far away from home, it is more likely that they will own more than a car. Other causes of endogeinty can be: how developed the public transportation network is or how strong is the driving culture in a country. Given a country with a well developed public transportation system, the sensibility of people for increases in gasoline price will be high. On the other hand, given a strong car driving culture, the sensibility of people for increases in gasoline price will be low. Since the assumptions are not likely to be fulfilled, the OLS estimator will be biased.

### (d) 
Truth:    
\begin{align*}
y = X\beta + (\gamma)\alpha + \epsilon
\end{align*}


Paper estimate:    
\begin{align*}
y = X\beta + \epsilon 
\end{align*}

Thus,

\begin{align*}
E(\hat\beta) &= \beta + \gamma(X'X)^{-1}X'\alpha \\
&= \beta + \gamma \hat{\beta_{\alpha-on-X}}
\end{align*}

Where $\gamma$ is the (implicit) sign of impact of $\alpha_i$ on $y_i$ and $\hat\beta_{\alpha-on-X}$ measures the correlation between $\alpha_i$ and $x_i$
The direction can be both positive and negative depending on the OVB we analize. 

1. Negative bias
In case the unobserved factor is how well developed public transportation network is in a country: 

- $\gamma$ will be negative -> the more developed the network is, the less people will use the car and also less will be the consumption of gasoline
- $\hat\beta_{\alpha-on-X}$ will be positive -> an increase in gasoline price will result in more people using the public transportation, so higher revenues for public transportation companies. They will invest more in the development of the network, leading to an increase in a better developed public transportation network. 

2. Positive bias
In case the unobserved factor is how strong the driving culture in a country:

- $\gamma$ will be positive -> the stronger the driving culture, the more people will drive, the more the consume in gasoline per car
- $\hat\beta_{\alpha-on-X}$ will be positive -> the stronger the driving culture, the more people will drive. This leads to an higher demand for gasoline which will raise the price of the gasoline. 

###(e)
####(e.i) The LSDV EStimator
```{r}
# LSDV Model
lsdv.model <- lm(c ~ y+p+car + as.factor(co), data=pdata, na.action=na.omit)
```
####(e.ii) Fixed Effects
```{r}
# Fixed effects or within estimator
fixed <- plm(c ~ y+p+car, data=pdata, model= "within")
```
####(e.iii) Random Effects
```{r}
# Random effects estimator
random <- plm(c ~ y+p+car, data=pdata, model= "random")
```

```{r tab6, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer(pooling, lsdv.model,fixed, random, type="latex", title="Comparison of Results",
          dep.var.labels=c("Gas per car consumption"),
          covariate.labels=c("Per capita income", "Gasoline price","Cars per capita"), align=TRUE)
```


```{r, include=FALSE}

# Between estimator
between <- plm(c ~ y+p+car, data=pdata, model= "between")
summary(between)

# First differences estimator
firstdiff <- plm(c ~ y+p+car, data=pdata, model= "fd")
summary(firstdiff)

# Fixed effects or within estimator
fixed <- plm(c ~ y+p+car, data=pdata, model= "within")
summary(fixed)

# Random effects estimator
random <- plm(c ~ y+p+car, data=pdata, model= "random")
summary(random)

# LM test for random effects versus OLS
plmtest(pooling)

# LM test for fixed effects versus OLS
pFtest(fixed, pooling)

# Hausman test for fixed versus random effects model
phtest(random, fixed)

# LSDV Model
lsdv.model <- lm(c ~ y+p+car + as.factor(co), data=pdata, na.action=na.omit)
summary(lsdv.model)


```

```{r tab7, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer(pooling, lsdv.model,fixed, type="latex", title="Pooled OLS Results",
          dep.var.labels=c("Gas per car consumption"),
          covariate.labels=c("Per capita income", "Gasoline price","Cars per capita"), align=TRUE)
```
