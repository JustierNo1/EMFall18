---
title: "Problem Set 2 Empirical Methods"
author: "Patrick Glettig (13-252-143), Carmine Ragone (14-995-021)"
date: "05.11.2018"
output:
  html_document: default
  pdf_document:
    extra_dependencies: ["dcolumn"]
---

```{r global options, include=FALSE}
library(knitr)
library(Cairo)
library(extrafont)
library(latex2exp)
library(readstata13)
library(Hmisc)
library(lmtest)
library(stargazer)
library(kableExtra)
library(ggplot2)
library(data.table)
library(tidyr)
library(formatR)
library(dplyr)

#install.packages("pacman")
#pacman::p_load(knitr, Cairo, extrafont,latex2exp,readstata13,Hmisc,lmtest,stargazer,kableExtra,ggplot2,data.table,tidyr,formatR,dplyr) 


knitr::opts_chunk$set(echo = TRUE, fig.show = "hold", collapse = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE,
                      fig.height = 2, fig.width = 4)

sysinf <- Sys.info()
if(sysinf['sysname'] == "Windows"){
  extrafont::loadfonts(device="win")
  #opts_chunk$set(dev = "CairoPDF")
}
```


# Paper and Pencil Questions

*(a) Show that* $TSS = ESS + RSS$*. Also, show you can write* $R^2 = 1-\frac{e'e}{\tilde{y}'\tilde{y}}$ *where* $\tilde{y}_i=y_i-\bar{y}$.

\begin{align*}
\sum_{i=1}^n{(y_i-\bar{y})^2}&=\sum_{i=1}^n{(y_i-\hat{y}_i-\bar{y})^2} \\ &=\sum_{i=1}^n{(y_i-\hat{y}_i)^2}+2\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}+\sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}
\end{align*}

We need to show that $2\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}=0$. By the regression equation we know that:

$$SSE=\sum_{i=1}^n{(y_i-\hat{y}_i)^2}=\sum_{i=1}^n{(y_i-\beta_0-\beta_1x_i)^2}$$
As we minimize SSE to find $\beta_1,\beta_0$ we search for the partial derivatives:

\begin{align*}
\frac{\delta SSE}{\delta \beta_0}&=\sum_{i=1}^n{2(y_i-\beta_0-\beta_1x_i)(-1)} \overset{!}{=}0 \\
\sum_{i=1}^n{\beta_0} &= \sum_{i=1}^n{y_i}-\beta_1 \sum_{i=1}^n{x_i} \Rightarrow 0=\sum_{i=1}^n{(y_i-\hat{y}_i)} \tag{a} \\
n\beta_0 &= \sum_{i=1}^n{y_i}-\beta_1 \sum_{i=1}^n{x_i} \\
\beta_0 &= \frac{1}{n}\sum_{i=1}^n{y_i} - \beta_1 \frac{1}{n} \sum_{i=1}^n{x_i}
\end{align*}

We take the second partial derivative:

\begin{equation}
\frac{\delta SSE}{\delta \beta_1} = \sum_{i=1}^n{2(y_i-\beta_0-\beta_1x_i)(-x_i)} \overset{!}{=}0  \\
\sum_{i=1}^n{2(y_i-\beta_0-\beta_1x_i)} = 0
\end{equation}

By the regression equation $\hat{y}_i=\beta_0 + \beta_1x_i$, thus $\frac{\hat{y}_i-\beta_0}{\beta_1}$ and so:

\begin{equation}
\sum_{i=1}^n{x_i (y_i-\hat{y}_i)} = 0 \\
\sum_{i=1}^n{\left(\frac{\hat{y}_i-\beta_0}{\beta_1}\right)(y_i-\hat{y}_i)}=0 \\
\frac{1}{\beta_1} \sum_{i=1}^n{\hat{y}_i(y_i-\hat{y}_i)} - \frac{\beta_0}{\beta_1} \sum_{i=1}^n{(y_i-\hat{y}_i)}=0
\end{equation}

Using equation (a) in the second part:

\begin{equation}
\sum_{i=1}^n{\hat{y}_i(y_i-\hat{y}_i)} = 0 \tag{b}
\end{equation}

We needed to show that $2\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}=0$:

$$\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}=\sum_{i=1}^n{\hat{y}_i(y_i-\hat{y}_i)}- \bar{y}\sum_{i=1}^n{(y_i-\hat{y}_i)}$$
Where the first part is 0 because of (b) and the second part because of (a).

For the second part of the question, we know that $e'e=\sum{e_i^2}=RSS$ and $\tilde{y}'\tilde{y}=\sum{y_i-\bar{y}}=TSS$. As $ESS=TSS-RSS$ and $TSS=RSS+ESS$ this becomes:

\begin{align*}
R^2 &= \frac{ESS}{TSS} \\
&= \frac{TSS-RSS}{TSS} \\
&= \frac{TSS}{TSS}-\frac{RSS}{TSS} \\
&= 1- \frac{RSS}{TSS}
\end{align*}

*(b) Show that* $R^2 = corr^2(y, \hat{y})$. *What is the intuition behind it?*

\begin{align*}
corr^2(y, \hat{y}) &= \left(\frac{Cov(y,\hat{y})}{\sqrt{Var(y)Var(\hat{y})}}\right)^2 \\
&= \frac{(Cov(y,\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Cov(\hat{y}+e,\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Cov(\hat{y},\hat{y})+Cov(\hat{y},e))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Cov(\hat{y},\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Var(\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{Var(\hat{y})}{Var(y)}=\frac{ESS}{TSS}=R^2
\end{align*}

The intuition behind this is how much of the total variance can be explained with our model that yields $Var(\hat{y})$.

*(c) Suppose you decided to measure all of your* $X$ *variables in different units such that your new* $X$ *variable, call it* $\tilde{X}$, *is exactly double your old one, i.e.* $\tilde{X} = 2X$. *Suppose you run the regression of* $y$ *on* $\tilde{X}$;  *call the resulting estimate* $\tilde{\beta}$. *You showed in Problem Set 1 that *$\tilde{\beta}=\frac{1}{2}\hat{\beta}$. *Is the $R^2$ different in the two models? Provide an intuitive answer.*

We have seen above that $R^2=\frac{Var(\hat{y})}{Var(y)}$. The denominator will not change as $y$ is not affected. $Var(\hat{y})$ is given by the model $\hat{y}=\beta_0+\tilde{\beta}x$. Using the hint from Problem Set 1, $\hat{y}=\beta_0+\frac{1}{2}\hat{\beta}2x$, which yields the same model as before. In other words, the new estimator $\tilde{\beta}$ adjusts for the new units in $X$. Thus $R^2$ is not going to change.

*(d) Intuitively discuss the fact that including another regressor in the linear model always decreases the* $RSS$.

Intuitively, if I keep adding another regressor in the linear model our model the variance left (which is approximated by the $RSS$) will be less. More formally, by adding another variable, we add another covariance to the model which will mechanically always increase $Var(\hat{y})$ and hence $ESS$. As $RSS=TSS-ESS$, this will always decrease $RSS$.

*(e) Provide a formal proof of point (d).*

$$R^2=\frac{\sum_{i=1}^n{(\hat{y}_i-\bar{y})}}{\sum_{i=1}^n{(y_i-\bar{y})}}$$
$y,\bar{y}$ is not affected by a change in $x$. So we focus on $\hat{y}$:

\begin{align*}
\hat{y}&=\beta_0+\tilde{\beta}x \\
\hat{y}&=\beta_0+\frac{1}{2}\hat{\beta}2x
\end{align*}

So $\hat{y}$ remains unchanged and none of the variables in the equation for $R^2$ changes.

*(f) Can you suggest a problem of interpreting the* $R^2$ *as a measure of how “good” the model is? If you think the model might not be “good”, why might it nevertheless have a high* $R^2$*?*

A good model identifies causal relationships between the dependent variable $y$ and the independent variables $x$. By adding more independent variables, we increase $R^2$ (seen above), but this tells us nothing about the causal relationship between $y$ and $x$. To identify causal relationships, we need $x$ variables with robust standard errors, however $R^2$ is not affected by the errors in $x$. This means we can have a "bad" model which still has a high $R^2$. The problem is that $R^2$ tells us how well our model fits the sample. If we only try to increase $R^2$, we have a tendency to overfit the model to the sample at hand, but might ignore the causal relationships of the true population.

# The gender wage gap

*Suppose you want to test whether in your country women are discriminated against  relative to men in terms of wages. You decide that you want to test whether men and women have different salaries. Suppose you are able to gather data on the whole working population in your country. For each individual you have the following information.*

* *monthly wage*
* *gender*
* *years of education*

*(a) Suppose years of education have the same effect on wages for both men and women. Propose a simple regression model to test your hypothesis.*
$$ Wage_i = \alpha + \beta_1Edu_i + \beta_2Male_i + \epsilon_i$$
Where *Male* is a dummy variable which takes value 0 for females and 1 for male observations. 

*(b) Provide a graphical representation of the conditional expectation function (i.e. the part of wages that we can explain with our covariates) and show if and how it differs for men and women.*

$$E(Wage_i|Edu_i,Male_i)=\begin{cases}
    \alpha + \beta_1Edu_i + \epsilon_i,& \text{if } Male_i = 0\\
     \alpha  + \beta_1Edu_i + \beta_2 + \epsilon_i,& \text{if } Male_i = 1
\end{cases}$$

*(c) In retrospect, you decide that years of education might have a different marginal effect on men compared to women. How would you modify your regression model to account for this differential effect?*

$$ Wage_i = \alpha + \beta_1Edu_i + \beta_2Male_i + \beta_3Edu_i \times Male_i + \epsilon_i $$

*(d) Provide a graphical representation of the conditional expectation function (i.e. the part of wages that we can explain with our covariates) and show if and how it differs for men and women*
$$E(Wage_i|Edu_i,Male_i)=\begin{cases}
    \alpha + \beta_1Edu_i + \epsilon_i,& \text{if } Male_i = 0\\
     \alpha  + \beta_1Edu_i + \beta_2 + \beta_3Edu_i + \epsilon_i,& \text{if } Male_i = 1
\end{cases}$$

# Empirical Application
**1. The Gender Wage Gap.** *In this exercise we will try to explore some discrimination theories analyzing a subsample from the US CPS2015. Many politicians, institutional observers, and researchers still claim today the existence of discrimination against female workers in the labor market. They base their claims looking at the gender wage gap, i.e. the difference between men’s and women’s wages. As many other things in
economics, this wage gap can be generated both from the demand side (employers who discriminate against women) and from the supply side (women having different skills or preferences for specific jobs or for entering the labor market at all). In this exercise we will try to learn more about the gender wage gap, while testing you on your econometric toolkit. For this question, assume that Assumption 2 (Mean-zero Error) holds so that you can make causal statements in your answers.*

*Download the dataset sampleUScens2015.csv from OLAT and import it into Stata or R. The dataset includes prime age individuals (i.e. *age $\in [25; 54]$) *active in the labor market (i.e. either employed or looking for job), and working in the private sector. There are seven relevant variables:*

* *age*, the age of the individual in 2015
* *education*, years of completed education
* *incwage*, income from wages in 2015 in USD
* *female*, dummy for female
* *childrenly*, dummy if had a children in the last year
* *degfield*, field of degree
* *occupation*, sector of occupation

At first we need to load the data in order to start our analysis:
```{r}
USCPS <- fread('sampleUScens2015.CSV')
head(USCPS) # Check the format
dim(USCPS) #Check length
summary(USCPS) #Explore the data
```
The summary tells us that there are some NAs in $educ$, we want to explore this:
```{r}
head(USCPS[rowSums(is.na(USCPS)) > 0,])
```
The other columns seem fine, we could still keep these observations in the dataset. However, the data quality on these 45 observations seems poor anyway as $firmsector, occupation$ and $degfield$ are in most cases "other", which is not really informative. Also, 45 observations are just a tiny fraction of the overall sample. As it is easier to work with a complete dataframe, we drop the observations containing NAs:
```{r results='hide'}
USCPS <- USCPS %>% drop_na()
```

(a) *Generate a new variable called* $wage = incwage/1000$. *Also, generate* $lw$ *taking the log of wage. Generate a dummy named university which is equal to 1 if* $education \geq 16$. *First regress wage on education, then regress wage on education and the university dummy. How does the coefficient on education change? How do you interpret it in both specifications?*

**Create the variables**
```{r}
USCPS$wage <- USCPS$incwage/1000 #create wage
USCPS$lw <- log(USCPS$wage) #create log wage
USCPS$university <- as.numeric(USCPS$educ >= 16) #create university dummy

```

**Run the regressions**
```{r, message=FALSE, warning=FALSE, results='asis'}
Model1 = wage ~ educ
lm1 <- lm(Model1, data = USCPS, x=TRUE)
Model2 = wage ~ educ + university
lm2 <- lm(Model2, data = USCPS, x=TRUE)

stargazer(lm1, lm2, title="Regression Results", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Wage"),
 covariate.labels=c("Education","University x Education"),
 omit.stat=c("all"), no.space=TRUE)

```
Without the university dummy, a 1 year increase in education is reflected by a wage increase of 7.074 USD. By adding the interaction term $University \times Education$ we separate the effect of years of education from years of university education. This results in a decreased coefficient for $Education$, 4.975 wheras the interaction term tells that a 1 year increase in university education is reflected by a wage increase of 15.18 USD. So people who attended university have a wage increase of 15.18 USD for an extra year of education whereas people without tertiary education only have an increase of 4.975 USD.

*(b) Drop the university dummy. Now regress wage on education and age. Also, regress log wages (lw) on education and age. What are their coefficients? How do you interpret them? How do they compare? [Note: be sure you compare approximately equivalent objects from each specification.]*
```{r, message=FALSE, warning=FALSE, results='asis'}
Model3 = wage ~ educ + age
lm3 <- lm(Model3, data = USCPS, x=TRUE)
Model4 = lw ~ educ + age
lm4 <- lm(Model4, data = USCPS, x=TRUE)

stargazer(lm3, lm4, title="Regression Results", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Wage","Log Wage"),
 covariate.labels=c("Education","Age"),
 omit.stat=c("all"), no.space=TRUE)
```
**Interpretation Model 1:**One year increase in education controlling for age increases wage by 7.17USD. One year increase in age controlling for education increases wage by 1.461USD.
**Interpretation Model 2:**One year increase in education controlling for age increases wage by 12%. One year increase in age controlling for education increases wage by 2.5%.

Altough their interpretation is different, they are in fact the same model. We can see this by comparing the sum of the fitted values:
```{r}
sum(fitted.values(lm3))
sum(fitted.values(lm4))
```

*(c) Now regress log wages on education, age, and the female dummy. You get the following model:*
$$lw_i = \beta_1 + \beta_2educ_ii + \beta_3age_i + \beta_4female_i + \epsilon_i$$

*What is the coefficient on female? How do you interpret it? Is it economically significant in your opinion? Test both in R/Stata and “by hand” the hypothesis that* $\beta_4 = 0$. *Should you use a one-sided or two-sided test? Do the one you think most appropriate.*
```{r, message=FALSE, warning=FALSE, results='asis'}
Model5 = lw ~ educ + age + female
lm5 <- lm(Model5, data = USCPS, x=TRUE)

stargazer(lm5, title="Regression Results", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Log Wage"),
 covariate.labels=c("Education","Age","Female"),
 no.space=TRUE, p.auto = TRUE)
```

Females earn on average 44.25% less than males with the same education and age. This is economically significant, by using the average wage and multiplying with the female coefficient we get `r mean(USCPS$wage)*0.4425`, a substantial difference.

To test in R whether $\beta_4 = 0$, we can simply look at the p-value of the female coefficient and see that is well below 5%. Using $\alpha=5\%$, we can state that $\beta_4$ is significantly different from 0. 

By hand:
$$ H_0: \beta_4 = 0\space and\space H_1: \beta_4 \neq 0$$
Calculate degrees of freedom:
$$N-K=561076-4=561072$$
Construct test statistic:

$$\frac{\hat{\beta_4}-\beta_4}{stderr(\hat{\beta_4})}=\tilde{t}=\frac{-0.4427-0}{0.0024}=-184,46$$
Now we look up in a t-table to find the critical value to find that the critical value $\bar{t}_{-184.46}=-1.967$ for $\alpha=5\%$. As $-184,46<-1.967$, we can reject $H_1$ and state that the coefficient is significantly different from 0 at the 5% confidence level. We used a two-sided test here because the question was to identify if the effect is different from 0, which can go both ways. 

*(d)* *Use R/Stata to get $\beta_4$ (the coefficient on female) using partitioned regression as we did in lecture.* \\ 

Given that: 

\begin{align*}
Y &=\beta X +\epsilon_{i} \\ &=X_k \beta_{k} + X_{-k} \beta_{-k} +\epsilon_{i}
\end{align*}

The idea of partitionated regression is to show that The $k^th$ coefficient in a multiple OLS regression is equivalent to the
coefficient in a simple OLS regression of the y on the residual from a regression of $X_k$ on all the other regressions. In this case, the first step is to regress the fact of being femail on the education and age to see how much of the variance is explained by those. 
```{r}
Model6 = female ~ educ + age
lm6 <- lm(Model6, data = USCPS, x=TRUE)

stargazer(lm6, title="Regression Results", type="latex", header = FALSE,
          align=TRUE, dep.var.labels=c("Female"),
          covariate.labels=c("Education","Age"),
          no.space=TRUE, p.auto = TRUE)

```
Log wage is regressed on the residuals of the last regression and it is possible to see that in both Model 7 and Model 5 the coefficient of the female and of the residuals is the same. 
```{r}
fem_res <- residuals(lm6) # Save the female residual values

Model7 = lw ~ fem_res
lm7 <- lm(Model7, data = USCPS, x=TRUE)
stargazer(lm7, title="Regression Results", type="latex", header = FALSE,
          align=TRUE, dep.var.labels=c("Log Wage"),
          covariate.labels=c("Female Residuals"),
          no.space=TRUE, p.auto = TRUE)
```
This is the demonstration that the only variation left in the variable "female to identify its coefficient is the variation left after running a regression of female on education and age.
