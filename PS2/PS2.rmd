---
title: "Problem Set 2 Empirical Methods"
author: "Patrick Glettig (13-252-143), Carmine Ragone (14-995-021)"
date: "05.11.2018"
output:
  pdf_document:
    extra_dependencies: ["dcolumn"]
  html_document: default
---

```{r global options, include=FALSE}
library(knitr)
library(Cairo)
library(extrafont)
library(latex2exp)
library(readstata13)
library(Hmisc)
library(lmtest)
library(stargazer)
library(kableExtra)
library(ggplot2)
library(data.table)
library(tidyr)
library(formatR)
library(dplyr)

knitr::opts_chunk$set(echo = TRUE, fig.show = "hold", collapse = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE,
                      fig.height = 2, fig.width = 4)

sysinf <- Sys.info()
if(sysinf['sysname'] == "Windows"){
  extrafont::loadfonts(device="win")
  #opts_chunk$set(dev = "CairoPDF")
}
```


# Paper and Pencil Questions

*(a) Show that* $TSS = ESS + RSS$*. Also, show you can write* $R^2 = 1-\frac{e'e}{\tilde{y}'\tilde{y}}$ *where* $\tilde{y}_i=y_i-\bar{y}$.

\begin{align*}
\sum_{i=1}^n{(y_i-\bar{y})^2}&=\sum_{i=1}^n{(y_i-\hat{y}_i-\bar{y})^2} \\ &=\sum_{i=1}^n{(y_i-\hat{y}_i)^2}+2\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}+\sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}
\end{align*}

We need to show that $2\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}=0$. By the regression equation we know that:

$$SSE=\sum_{i=1}^n{(y_i-\hat{y}_i)^2}=\sum_{i=1}^n{(y_i-\beta_0-\beta_1x_i)^2}$$
As we minimize SSE to find $\beta_1,\beta_0$ we search for the partial derivatives:

\begin{align*}
\frac{\delta SSE}{\delta \beta_0}&=\sum_{i=1}^n{2(y_i-\beta_0-\beta_1x_i)(-1)} \overset{!}{=}0 \\
\sum_{i=1}^n{\beta_0} &= \sum_{i=1}^n{y_i}-\beta_1 \sum_{i=1}^n{x_i} \Rightarrow 0=\sum_{i=1}^n{(y_i-\hat{y}_i)} \tag{a} \\
n\beta_0 &= \sum_{i=1}^n{y_i}-\beta_1 \sum_{i=1}^n{x_i} \\
\beta_0 &= \frac{1}{n}\sum_{i=1}^n{y_i} - \beta_1 \frac{1}{n} \sum_{i=1}^n{x_i}
\end{align*}

We take the second partial derivative:

\begin{equation}
\frac{\delta SSE}{\delta \beta_1} = \sum_{i=1}^n{2(y_i-\beta_0-\beta_1x_i)(-x_i)} \overset{!}{=}0  \\
\sum_{i=1}^n{2(y_i-\beta_0-\beta_1x_i)} = 0
\end{equation}

By the regression equation $\hat{y}_i=\beta_0 + \beta_1x_i$, thus $\frac{\hat{y}_i-\beta_0}{\beta_1}$ and so:

\begin{equation}
\sum_{i=1}^n{x_i (y_i-\hat{y}_i)} = 0 \\
\sum_{i=1}^n{\left(\frac{\hat{y}_i-\beta_0}{\beta_1}\right)(y_i-\hat{y}_i)}=0 \\
\frac{1}{\beta_1} \sum_{i=1}^n{\hat{y}_i(y_i-\hat{y}_i)} - \frac{\beta_0}{\beta_1} \sum_{i=1}^n{(y_i-\hat{y}_i)}=0
\end{equation}

Using equation (a) in the second part:

\begin{equation}
\sum_{i=1}^n{\hat{y}_i(y_i-\hat{y}_i)} = 0 \tag{b}
\end{equation}

We needed to show that $2\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}=0$:

$$\sum_{i=1}^n{(y_i-\hat{y}_i)(\hat{y}_i-\bar{y})}=\sum_{i=1}^n{\hat{y}_i(y_i-\hat{y}_i)}- \bar{y}\sum_{i=1}^n{(y_i-\hat{y}_i)}$$
Where the first part is 0 because of (b) and the second part because of (a).

For the second part of the question, we know that $e'e=\sum{e_i^2}=RSS$ and $\tilde{y}'\tilde{y}=\sum{y_i-\bar{y}}=TSS$. As $ESS=TSS-RSS$ and $TSS=RSS+ESS$ this becomes:

\begin{align*}
R^2 &= \frac{ESS}{TSS} \\
&= \frac{TSS-RSS}{TSS} \\
&= \frac{TSS}{TSS}-\frac{RSS}{TSS} \\
&= 1- \frac{RSS}{TSS}
\end{align*}

*(b) Show that* $R^2 = corr^2(y, \hat{y})$. *What is the intuition behind it?*

\begin{align*}
corr^2(y, \hat{y}) &= \left(\frac{Cov(y,\hat{y})}{\sqrt{Var(y)Var(\hat{y})}}\right)^2 \\
&= \frac{(Cov(y,\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Cov(\hat{y}+e,\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Cov(\hat{y},\hat{y})+Cov(\hat{y},e))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Cov(\hat{y},\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{(Var(\hat{y}))^2}{Var(y)Var(\hat{y})} \\
&= \frac{Var(\hat{y})}{Var(y)}=\frac{ESS}{TSS}=R^2
\end{align*}

The intuition behind this is how much of the total variance can be explained with our model that yields $Var(\hat{y})$.

*(c) Suppose you decided to measure all of your* $X$ *variables in different units such that your new* $X$ *variable, call it* $\tilde{X}$, *is exactly double your old one, i.e.* $\tilde{X} = 2X$. *Suppose you run the regression of* $y$ *on* $\tilde{X}$;  *call the resulting estimate* $\tilde{\beta}$. *You showed in Problem Set 1 that *$\tilde{\beta}=\frac{1}{2}\hat{\beta}$. *Is the $R^2$ different in the two models? Provide an intuitive answer.*

We have seen above that $R^2=\frac{Var(\hat{y})}{Var(y)}$. The denominator will not change as $y$ is not affected. $Var(\hat{y})$ is given by the model $\hat{y}=\beta_0+\tilde{\beta}x$. Using the hint from Problem Set 1, $\hat{y}=\beta_0+\frac{1}{2}\hat{\beta}2x$, which yields the same model as before. In other words, the new estimator $\tilde{\beta}$ adjusts for the new units in $X$. Thus $R^2$ is not going to change.

*(d) Intuitively discuss the fact that including another regressor in the linear model always decreases the* $RSS$.

By adding another variable, we add another covariance to the model which will mechanically always increase $Var(\hat{y})$ and hence $ESS$. As $RSS=TSS-ESS$, this will always decrease $RSS$.

*(e) Provide a formal proof of point (d).*

$$R^2=\frac{\sum_{i=1}^n{(\hat{y}_i-\bar{y})}}{\sum_{i=1}^n{(y_i-\bar{y})}}$$
$y,\bar{y}$ is not affected by a change in $x$. So we focus on $\hat{y}$:

\begin{align*}
\hat{y}&=\beta_0+\tilde{\beta}x \\
\hat{y}&=\beta_0+\frac{1}{2}\hat{\beta}2x
\end{align*}

So $\hat{y}$ remains unchanged and none of the variables in the equation for $R^2$ changes.

*(f) Can you suggest a problem of interpreting the* $R^2$ *as a measure of how “good” the model is? If you think the model might not be “good”, why might it nevertheless have a high* $R^2$*?*

A good model identifies causal relationships between the dependent variable $y$ and the independent variables $x$. By adding more independent variables, we increase $R^2$ (seen above), but this tells us nothing about the causal relationship between $y$ and $x$. To identify causal relationships, we need $x$ variables with robust standard errors, however $R^2$ is not affected by the errors in $x$. This means we can have a "bad" model which still has a high $R^2$. The problem is that $R^2$ tells us how well our model fits the sample. If we only try to increase $R^2$, we have a tendency to overfit the model to the sample at hand, but might ignore the causal relationships of the true population.

### 2. The gender wage gap

*Suppose you want to test whether in your country women are discriminated against  relative to men in terms of wages. You decide that you want to test whether men and women have different salaries. Suppose you are able to gather data on the whole working population in your country. For each individual you have the following information.*

* *monthly wage*
* *gender*
* *years of education*

*(a) Suppose years of education have the same effect on wages for both men and women. Propose a simple regression model to test your hypothesis.*
$$ Wage_i = \alpha + \beta_1Edu_i + \beta_2Male_i + \epsilon_i$$
Where *Male* is a dummy variable which takes value 0 for females and 1 for male observations. 

*(b) Provide a graphical representation of the conditional expectation function (i.e. the part of wages that we can explain with our covariates) and show if and how it differs for men and women.*

$$E(Wage_i|Edu_i,Male_i)=\begin{cases}
    \alpha + \beta_1Edu_i + \epsilon_i,& \text{if } Male_i = 0\\
     \alpha  + \beta_1Edu_i + \beta_2 + \epsilon_i,& \text{if } Male_i = 1
\end{cases}$$

*(c) In retrospect, you decide that years of education might have a different marginal effect on men compared to women. How would you modify your regression model to account for this differential effect?*

$$ Wage_i = \alpha + \beta_1Edu_i + \beta_2Male_i + \beta_3Edu_i \times Male_i + \epsilon_i $$

*(d) Provide a graphical representation of the conditional expectation function (i.e. the part of wages that we can explain with our covariates) and show if and how it differs for men and women*
$$E(Wage_i|Edu_i,Male_i)=\begin{cases}
    \alpha + \beta_1Edu_i + \epsilon_i,& \text{if } Male_i = 0\\
     \alpha  + \beta_1Edu_i + \beta_2 + \beta_3Edu_i + \epsilon_i,& \text{if } Male_i = 1
\end{cases}$$

## 2. Empirical Application
**1. The Gender Wage Gap.** *In this exercise we will try to explore some discrimination theories analyzing a subsample from the US CPS2015. Many politicians, institutional observers, and researchers still claim today the existence of discrimination against female workers in the labor market. They base their claims looking at the gender wage gap, i.e. the difference between men’s and women’s wages. As many other things in
economics, this wage gap can be generated both from the demand side (employers who discriminate against women) and from the supply side (women having different skills or preferences for specific jobs or for entering the labor market at all). In this exercise we will try to learn more about the gender wage gap, while testing you on your econometric toolkit. For this question, assume that Assumption 2 (Mean-zero Error) holds so that you can make causal statements in your answers.*

*Download the dataset sampleUScens2015.csv from OLAT and import it into Stata or R. The dataset includes prime age individuals (i.e. *age $\in [25; 54]$) *active in the labor market (i.e. either employed or looking for job), and working in the private sector. There are seven relevant variables:*

* *age*, the age of the individual in 2015
* *education*, years of completed education
* *incwage*, income from wages in 2015 in USD
* *female*, dummy for female
* *childrenly*, dummy if had a children in the last year
* *degfield*, field of degree
* *occupation*, sector of occupation

At first we need to load the data in order to start our analysis:
```{r}
USCPS <- fread('sampleUScens2015.CSV')
head(USCPS) # Check the format
dim(USCPS) #Check length
summary(USCPS) #Explore the data
```
The summary tells us that there are some NAs in $educ$, we want to explore this:
```{r}
head(USCPS[rowSums(is.na(USCPS)) > 0,])
```
The other columns seem fine, we could still keep these observations in the dataset. However, the data quality on these 45 observations seems poor anyway as $firmsector, occupation$ and $degfield$ are in most cases "other", which is not really informative. Also, 45 observations are just a tiny fraction of the overall sample. As it is easier to work with a complete dataframe, we drop the observations containing NAs:
```{r results='hide'}
USCPS <- USCPS %>% drop_na()
```

(a) *Generate a new variable called* $wage = incwage/1000$. *Also, generate* $lw$ *taking the log of wage. Generate a dummy named university which is equal to 1 if* $education \geq 16$. *First regress wage on education, then regress wage on education and the university dummy. How does the coefficient on education change? How do you interpret it in both specifications?*

**Create the variables**
```{r}
USCPS$wage <- USCPS$incwage/1000 #create wage
USCPS$lw <- log(USCPS$wage) #create log wage
USCPS$university <- as.numeric(USCPS$educ >= 16) #create university dummy

```

**Run the regressions**
```{r, message=FALSE, warning=FALSE, results='asis'}
Model1 = wage ~ educ
lm1 <- lm(Model1, data = USCPS, x=TRUE)
Model2 = wage ~ educ + university
lm2 <- lm(Model2, data = USCPS, x=TRUE)

stargazer(lm1, lm2, title="Regression Results", type="latex", header = FALSE,
 align=TRUE, dep.var.labels=c("Wage"),
 covariate.labels=c("Education","University x Education"),
 omit.stat=c("all"), no.space=TRUE, ci = TRUE)

```
Without the university dummy, a 1 year increase in education is reflected by a wage increase of 7.074 USD. By adding the interaction term $University \times Education$ we separate the effect of years of education from years of university education. This results in a decreased coefficient for $Education$, 4.975 wheras the interaction term tells that a 1 year increase in university education is reflected by a wage increase of 15.18 USD. So people who attended university have a wage increase of 15.18 USD for an extra year of education whereas people without tertiary education only have an increase of 4.975 USD.


